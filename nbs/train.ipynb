{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, GPT2LMHeadModel, AutoTokenizer, TextDataset, AutoModelForCausalLM\n",
    "import datasets\n",
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load dataset \n",
    "train_dataset = load_from_disk(\"article_512/train\")\n",
    "test_dataset = load_from_disk(\"article_512/test\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"imthanhlv/vigpt2medium\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"imthanhlv/vigpt2medium\")\n",
    "special_tokens_dict = {'additional_special_tokens': ['<|beginofdes|>','<|endofdes|>', '<br>']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Data collator for padding\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"pt\")\n",
    "\n",
    "batch_size = 4\n",
    "gradient_accumulation_steps = 2\n",
    "num_gpu = 1\n",
    "\n",
    "step_per_batch = math.ceil(len(train_dataset) / (batch_size * gradient_accumulation_steps * num_gpu))\n",
    "print('step_per_batch', step_per_batch)\n",
    "\n",
    "def train(model):\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='training_article',\n",
    "        # group_by_length=True,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        num_train_epochs=30,\n",
    "        # fp16=True,\n",
    "        save_steps=step_per_batch,\n",
    "        eval_steps=step_per_batch,\n",
    "        logging_steps=step_per_batch,\n",
    "        learning_rate=5e-3,\n",
    "        warmup_steps=step_per_batch * 5,\n",
    "        save_total_limit=5,\n",
    "        load_best_model_at_end=True,\n",
    "        prediction_loss_only=True,\n",
    "        metric_for_best_model='loss',\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        data_collator=data_collator,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience = 4)]\n",
    "    )\n",
    "    trainer.place_model_on_device = False\n",
    "    trainer.train()\n",
    "    trainer.save_model(\"training_article/best_model\")\n",
    "    tokenizer.save_pretrained(\"training_article/best_model\")\n",
    "\n",
    "train(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_repo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
